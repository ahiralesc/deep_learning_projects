{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "\n",
    "El reto supone la construcción de un autocodificador **determinístico convolucional**, es decir:\n",
    "- Existe un espacio latente único. Por lo que, para cada entrada existe solo un punto en el espacio latente (relación bionivoca)\n",
    "- No se introduce ruido ni distribuciones de probabilidad en el espacio latente.\n",
    "- El autocodificador solo considera capas densas o convolucionales, entrenandas con pérdida como MSE o BCE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Competencias:\n",
    "1. Diseñar e implementar un autocodificador utilizando una arquitectura basada en clases. \n",
    "2. Aplicar principios de redes convolucionales (CNN) y feedforward para diseñar el codificador y el decodificador.\n",
    "3. Dividir el conjunto de datos en dos subconjuntos: conjunto de entrenamiento (80%) y conjunto de prueba (20%).\n",
    "4. Aplicar y analizar el desempeño de diferentes métodos de optimización (descenso de gradiente, Adam, RMSprop, u otros) y de diferentes funciones de pérdida (MSE, BCE, etc.) \n",
    "5. Desarrollar una métrica (distancia hamming) para evaluar la calidad del decodificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrucciones:\n",
    "- Identifique las secciones con la palabra **competencia** y reemplace la línea  ```______________``` con el código que corresponda.\n",
    "- Investigue que operadores se aplican para la construcción de una red de aprendizaje determinista (capa densa, funciones de activación).\n",
    "- Evalué diferentes métodos de optimización y funciones de pérdida.  \n",
    "- Investigue y desarrolle la métrica distancia hamming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reto 1. Descifrando del mensaje secreto.\n",
    "\n",
    "Un oponente genera mensajes de 32 bits con información confidencial y los cifra utilizando un codificador determinista que reduce cada mensaje a una secuencia de 16 números de punto flotante. De los 10,000 mensajes generados, se ha logrado interceptar solo el 30% de los originales (3000 mensajes) junto con sus correspondientes versiones cifradas (100 de ellos). Su misión es entrenar un decodificador capaz de reconstruir exactamente los mensajes originales a partir de sus versiones cifradas, utilizando únicamente los 3000 mensajes interceptados para el entrenamiento.<br>\n",
    "\n",
    "Los siguientes son ejemplos de mensajes no-cifrados confidenciales<br>\n",
    "```\n",
    "11000110100110011001100110010101\n",
    "10111010010100010110111001000110\n",
    "11001100110100000000100001010011\n",
    "10110001001010101101110100101000\n",
    "``` \n",
    "\n",
    "El siguiente es un ejemplo un mensaje cifrado<br>\n",
    "```\n",
    "-0.21161067  0.9819643   0.50920117 -0.10046072  0.0970166   0.47773603\n",
    "-0.02057732 -0.19037446  0.99944896  0.6825894  -0.1577787   0.3304235\n",
    "0.73278713 -0.09858703  0.11817224  0.9356269\n",
    "```\n",
    "\n",
    "El reto consiste en construir un decodificador capaz de recuperar los mensajes cifrados interceptados sin conocer los detalles de la red neuronal que los generó, incluyendo:\n",
    "- El número de capas.\n",
    "- La cantidad de nodos por capa.\n",
    "- Las funciones de activación utilizadas.\n",
    "- El número de épocas y el tamaño del batch en su entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías para aprendizaje\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Librerias para representación de datos y E/S\n",
    "import numpy as np\n",
    "from numpy import savetxt, loadtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Competencia 1**: Diseñar e implementar un autocodificador utilizando una arquitectura basada en clases. \n",
    "- **Competencia 2**: Aplicar principios de redes convolucionales (CNN) y feedforward para diseñar el codificador y el decodificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autocodificador(tf.keras.Model):\n",
    "    def __init__(self, long_msg):\n",
    "        super(Autocodificador, self).__init__()\n",
    "        \n",
    "        # Longitud del mensaje secreto\n",
    "        self.long_msg = long_msg\n",
    "        \n",
    "        # Capas del codificador (codif)\n",
    "        _______________________________\n",
    "       \n",
    "        # Capas del decodificador (decod)\n",
    "        _______________________________\n",
    "        \n",
    "    \n",
    "    def codificador(self, entradas):\n",
    "        \"\"\" Interfaz para acceder solo al codificador \"\"\"\n",
    "        # Aplicación de las capas de codificador\n",
    "        _______________________________\n",
    "    \n",
    "        return codificado\n",
    "\n",
    "    \n",
    "    def decodificador(self, codificado):\n",
    "        \"\"\" Interfaz para acceder solo al decodificador \"\"\"\n",
    "        # Aplicación de las capas de decodificado\n",
    "         _______________________________\n",
    "            \n",
    "        return decodificado    \n",
    "    \n",
    "    \n",
    "    # Con training=True (modo entrenamiento) capas como:\n",
    "    # - Dropout aplican aleatoriamente el apagado de neuronas.\n",
    "    # - BatchNormalization usa la media y varianza del mini-lote en curso.\n",
    "    #  Esta implementación no las aplica por ende training=False por omision.\n",
    "    def call(self, entradas, training=False):\n",
    "        codificado = self.codificador(entradas)  # Salida del codificador\n",
    "        decodificado = self.decodificador(codificado)  # Salida del decodificador\n",
    "        return decodificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = Autocodificador(long_msg)\n",
    "modelo(tf.keras.Input(shape=(long_msg,)))  \n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_msg = 64 \n",
    "\n",
    "# Cargado de las 3000 cadenas interceptadas.\n",
    "datos = loadtxt(\"mensajes_interceptados.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Competencia 3**: Dividir el conjunto de datos en dos subconjuntos: conjunto de entrenamiento (80%) y conjunto de prueba (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividido en conjuntos de entrenamiento y prueba (80 % de entrenamiento, 20 % de prueba)\n",
    "\n",
    "# Estime un humbrar para la selección del 80% de datos \n",
    "# para el proceso de entrenamiento\n",
    "entrenamiento_tamano = _______________________________   \n",
    "# Seleccione el 80% de datos en terminos del humbral previmente estimado\n",
    "prueba_datos = _______________________________\n",
    "\n",
    "# Convertir a conjuntos de datos TensorFlow\n",
    "cargador_entrenamiento = tf.data.Dataset.from_tensor_slices(entrenamiento_datos)\n",
    "cargador_prueba        = tf.data.Dataset.from_tensor_slices(prueba_datos)\n",
    "\n",
    "# Defina el tamaño del batch\n",
    "tamano_batch = _______________________________ \n",
    "\n",
    "# Aplicar mezcla, procesamiento por lotes y precarga al conjunto de entrenamiento\n",
    "cargador_entrenamiento = cargador_entrenamiento.shuffle(buffer_size=10000).batch(tamano_batch).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Aplicar procesamiento por lotes y precarga al conjunto de pruebas\n",
    "cargador_prueba = cargador_prueba.batch(tamano_batch).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Competencia 4**: Aplicar y analizar el desempeño de diferentes métodos de optimización (descenso de gradiente, Adam, RMSprop, u otros) y de diferentes funciones de pérdida (MSE, BCE, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de épocas para entrenar el modelo\n",
    "\n",
    "num_epocas = _______________________________       # Defina el número de epocas\n",
    "\n",
    "# Definir el optimizador y la función de pérdida\n",
    "optimizador = _______________________________      # Defina el optimizador\n",
    "funcion_perdida =  _______________________________ # Defina la función de pérdida.\n",
    "\n",
    "# Iterar sobre las épocas\n",
    "for epoch in range(1, num_epocas + 1):\n",
    "    \n",
    "    # Registro de la pérdida de entrenamiento\n",
    "    perdida_accum = 0.0\n",
    "\n",
    "    ###################\n",
    "    # Entrenar el modelo #\n",
    "    ###################\n",
    "    for batch_datos in cargador_entrenamiento:\n",
    "        # El autocodificador espera una entrada de tamaño `long_msg`\n",
    "        # Asegúrate de que los datos tengan la forma adecuada\n",
    "        batch_datos = tf.reshape(batch_datos, (-1, long_msg))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Pase hacia adelante: calcula las salidas predichas pasando las entradas al modelo\n",
    "            salidas = modelo(batch_datos)\n",
    "            # Calcula la pérdida\n",
    "            perdida = funcion_perdida(batch_datos, salidas)\n",
    "\n",
    "        # Pase hacia atrás: calcula el gradiente de la pérdida con respecto a los parámetros del modelo\n",
    "        gradients = tape.gradient(perdida, modelo.trainable_variables)\n",
    "        # Realizar un paso de optimización (actualización de parámetros)\n",
    "        optimizador.apply_gradients(zip(gradients, modelo.trainable_variables))\n",
    "\n",
    "        # Actualizar la pérdida de entrenamiento acumulada\n",
    "        perdida_accum += perdida.numpy() * batch_datos.shape[0]\n",
    "\n",
    "    # Imprimir estadísticas de entrenamiento promedio\n",
    "    perdida_accum = perdida_accum / len(cargador_entrenamiento)\n",
    "    print(f'Epoca: {epoch} \\tPerdida de entrenamiento: {perdida_accum:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Competencia 5**: Desarrollar una métrica (distancia hamming) para evaluar la calidad del decodificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la distancia Hamming entre los mensajes reales y los descifrados\n",
    "# distancia hamming. En este caso, si los mensajes son perfectamente \n",
    "# descifrados la distancia hamming será cero.\n",
    "\n",
    "def hamming( x, y ):\n",
    "    _______________________________\n",
    "    \n",
    "    return distancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los 100 mensajes interceptados para los cuales existe codificación\n",
    "mensajes_cifrados = loadtxt(\"mensajes_cifrados.csv\", delimiter=\",\")\n",
    "mensajes_originales = loadtxt(\"mensajes_originales.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los Codifica los mensajes interceptados\n",
    "mensajes_cifrados = np.empty((0, 16), dtype=np.float32)\n",
    "for mensaje in mensajes_interceptados:\n",
    "    msgCifrado = codificador(mensaje.reshape(1,-1))\n",
    "    mensajes_cifrados = np.vstack([mensajes_cifrados, msgCifrado])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Almacenar y cargar los mensages cifrados\n",
    "savetxt(\"mensajes_cifrados.csv\", mensajes_cifrados, delimiter=\",\")\n",
    "savetxt(\"mensajes_originales.csv\", mensajes_interceptados, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensajes_cifrados = loadtxt(\"mensajes_cifrados.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mensajes_decodificados = np.empty((0, long_msg), dtype=np.float32)\n",
    "for cifrado in mensajes_cifrados:\n",
    "    decodificado = decodificador(cifrado.reshape(1,-1))\n",
    "    decodificado = tf.cast(decodificado > 0.5, tf.float32)\n",
    "    mensajes_decodificados = np.vstack([mensajes_decodificados, decodificado])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distancia = 0\n",
    "for mensaje, decodificado in zip(mensajes_originales, mensajes_decodificados):\n",
    "    mensaje = ''.join(str(int(bit)) for bit in mensaje)\n",
    "    decodificado = ''.join(str(int(bit)) for bit in decodificado)\n",
    "    entero_mensaje = int(mensaje,2)\n",
    "    entero_decodificado = int(decodificado, 2)\n",
    "    dist = hamming( entero_mensaje, entero_decodificado )\n",
    "    distancia += dist\n",
    "    # print(\"Mensaje original     : \", mensaje)\n",
    "    # print(\"Mensaje decodificado : \", decodificado)\n",
    "    # print(\"Distancia Hamming    : \", dist)\n",
    "distancia = distancia/len(mensajes_interceptados)\n",
    "print(\"La distancia promedio es: \", distancia)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neutrons",
   "language": "python",
   "name": "neutrons"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
